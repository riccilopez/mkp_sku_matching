{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65116dac-4563-4536-bdaa-6bf23ecde827",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Product matching \n",
    "### Install the requirements and load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3efc03e-5162-4bbb-b01d-f43f9d21bcac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the requirements\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc239111-055e-4057-b596-61f8736846e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "# Import the in-house libraries\n",
    "from modules.file_reader import MktpPricesFileReader, OnlineFileReader\n",
    "from modules.sku_matcher import get_confidence\n",
    "from modules.normalize_text import normalize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8fef9d-021d-4333-a9a6-6c4c89eca2ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructField, StringType,\\\n",
    "     FloatType, StructType, DateType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14cd8968-1631-43bb-8bd3-11ef7da6c566",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the input and output directories\n",
    "#MAIN_PATH = './data'\n",
    "MAIN_PATH = '/dbfs/mnt/webscraping'\n",
    "MKPT_PRICE_PATH        = f'{MAIN_PATH}/marketplace_prices/'\n",
    "WEBSCRAPING_INPUT_PATH = f'{MAIN_PATH}/webscraping_files/'\n",
    "MATCHING_OUPUT_PATH    = './data/matched_parquets/'\n",
    "\n",
    "# `date_sf` is formated as `dd-mm-yy`\n",
    "#date_sf  = '06-11-23'\n",
    "date_sf  = datetime.today().strftime('%d-%m-%y')\n",
    "country  = 'MX'\n",
    "database = 'deltadb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cdc742-cd92-4303-b2e4-8f094dc857b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mount the `webscraping` container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8b62f8-527d-4ff0-a391-0fa0daf03d28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Temporal cell\n",
    "# mount webscraping container\n",
    "# dbutils.fs.ls('/mnt/')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./credentials/azure_storage_conf.ini')\n",
    "\n",
    "storage_account_name = config['AzureBlobStorage']['account_name']\n",
    "storage_account_key  = config['AzureBlobStorage']['account_key']\n",
    "container_name       = config['AzureBlobStorage']['container_name']\n",
    "\n",
    "# Mount the container into databricks\n",
    "mount_path = f\"/mnt/{container_name}\"\n",
    "# check if the container is already mounted\n",
    "if not any(mount.mountPoint == mount_path for mount in dbutils.fs.mounts()):\n",
    "  dbutils.fs.mount(source = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/\",\n",
    "                   mount_point = mount_path,\n",
    "                   extra_configs = {\n",
    "                      f\"fs.azure.account.key.{storage_account_name}.blob.core.windows.net\": \n",
    "                      storage_account_key\n",
    "                    }\n",
    "                 )\n",
    "  print(f\"/mnt/{container_name} has just been mounted.\")\n",
    "else:\n",
    "  print(f\"Already mounted.\")\n",
    "# Check the mounted objects\n",
    "# dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac49fb11-6e20-4afa-af32-d0373b0aede3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load the input files\n",
    "### ABI Marketplace SKU-Price catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f2cc14-9e03-4bef-8da5-9b5fbe922505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mkp_delta_name = 'mktp_prices_catalog'\n",
    "\n",
    "# Get the file path by formating the date according to the naming convention\n",
    "# `country`-`YYYY`-`dd`-`mm` MX-2023-11-10.parquet\n",
    "date_YDM_formated = datetime.strptime(date_sf, \"%d-%m-%y\").strftime(\"%Y-%d-%m\")\n",
    "mktp_prices_path = Path(f'{MKPT_PRICE_PATH}/{country}/{country}-{date_YDM_formated}.parquet')\n",
    "\n",
    "# Read the parquet file using the `MktpPricesFileReader` class\n",
    "df_mkpt_prices = MktpPricesFileReader(mktp_prices_path).read_file()\n",
    "\n",
    "# Check that the given `date` is correct and unique\n",
    "# if not, raise an error and interrupt the process\n",
    "continue_falg = False\n",
    "try:\n",
    "    assert df_mkpt_prices['date'].nunique() == 1\n",
    "    assert df_mkpt_prices['date'].unique()[0].strftime(\"%d-%m-%y\") == date_sf\n",
    "except TypeError as e:\n",
    "    print(f'Process interrupted ({e}): The provided file was incorrect.')\n",
    "except AssertionError as e:\n",
    "    print(f'Process interrupted: The `date` column has an incorrect format.')\n",
    "else:\n",
    "    # Convert to pyspark dataframe\n",
    "    df_mkpt_prices.iteritems = df_mkpt_prices.items\n",
    "    df_mkpt_prices = spark.createDataFrame(df_mkpt_prices)\n",
    "    df_mkpt_prices = df_mkpt_prices.withColumn(\"date\", F.col(\"date\").cast(DateType()))\n",
    "    # display(df_mkpt_prices.show(3))\n",
    "    continue_falg = True\n",
    "\n",
    "# Continue\n",
    "assert continue_falg, 'Stopped do to incorrect execution.'\n",
    "\n",
    "# Check that the provided `country` and `date` does not already exists in the delta table\n",
    "df_all_mkp = spark.sql(\"select distinct `country`, `date` from deltadb.mktp_prices_catalog\")\n",
    "# Check wheter the country and date already exist\n",
    "country_exists    = (df_all_mkp.country == country)\n",
    "date_YMD_formated = datetime.strptime(date_sf, \"%d-%m-%y\").strftime(\"%Y-%m-%d\")\n",
    "date_exists       = (df_all_mkp.date == F.lit(date_YMD_formated))\n",
    "already_exists    = df_all_mkp.filter(country_exists & date_exists).count() > 0\n",
    "\n",
    "# Append the new data if not exists\n",
    "if not already_exists:\n",
    "    print(f\"Adding data into the delta table: `{mkp_delta_name}`\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database}\")\n",
    "    spark.sql(f\"USE {database}\")\n",
    "    df_mkpt_prices.write.format(\"delta\")\\\n",
    "                  .mode(\"append\").saveAsTable(mkp_delta_name)\n",
    "else:\n",
    "    print(f\"Data ({country}, {date_YMD_formated}) already exist (`{mkp_delta_name}`).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53f97c0-5d37-4f64-90a4-45498d67b292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the list of sku and sku_names\n",
    "# Define the query:\n",
    "mktp_cat_query = \"\"\"\n",
    "select \n",
    "  distinct country, sku, sku_name \n",
    "from deltadb.mktp_prices_catalog\n",
    "where date >= dateadd(day, -15, getdate()) \n",
    "      and date <= getdate();\n",
    "\"\"\"\n",
    "df_mktp_cat = spark.sql(mktp_cat_query)\\\n",
    "                   .dropDuplicates(['country', 'sku'])\n",
    "\n",
    "# Decorate the 'normalize_text' function\n",
    "@F.udf(StringType())\n",
    "def normalize_text_udf(text):\n",
    "    return normalize_text(text, encode='ascii')\n",
    "\n",
    "# Cleaning phase\n",
    "# Lowercase string fields\n",
    "df_mktp_cat = df_mktp_cat.withColumn(\n",
    "                        'mkp_sku_name_clean', \n",
    "                        normalize_text_udf(df_mktp_cat['sku_name']))\n",
    "df_mktp_cat.cache()#.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cedb92b-4e23-48ee-a942-e0b420f0d43b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load the web scraping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e5b92c-fa10-4a3f-bf4b-ac1dc4604700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to the web scraping directory\n",
    "scraped_path = Path(WEBSCRAPING_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c45548-938b-4d64-80c5-bcb97a8c40f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Date to match (corresponds to a single directory per date)\n",
    "scraping_date = date_sf # datetime.strptime(date_sf, \"%d-%m-%y\").strftime(\"%d%m%Y\")\n",
    "date_path = scraped_path / scraping_date \n",
    "\n",
    "# List of .txt files inside the date directory\n",
    "scraping_files = list(date_path.glob('*.txt'))\n",
    "\n",
    "# STOP the rest of process if there is no webscraping data\n",
    "assert len(scraping_files) > 0, f'There are no webscraping files for {date_sf} date.'\n",
    "\n",
    "scraping_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387d43d7-3e34-49d3-858a-2932807c0fcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the `OnlineFileReader` to load and clean the .txt files\n",
    "try:\n",
    "  df_scrap = pd.concat([OnlineFileReader(f).read_file() for f in scraping_files])\\\n",
    "                .reset_index(drop = True)\n",
    "except ValueError as e:\n",
    "  raise Exception(f'No webscraping data to process for {date_sf} date.')\n",
    "\n",
    "# Reset index index for future analaysis\n",
    "# df_scrap = df_scrap.reset_index(names = 'scrap_id')\n",
    "print('DataFrame dims.:', df_scrap.shape)\n",
    "\n",
    "# Convert to SparkDF\n",
    "# TODO: Move inside the `file_reader.py` module\n",
    "df_scrap.iteritems = df_scrap.items\n",
    "schema = StructType([ \\\n",
    "    StructField(\"type\",     StringType(), False), \\\n",
    "    StructField(\"country\",  StringType(), False), \\\n",
    "    StructField(\"locality\", StringType(), False), \\\n",
    "    StructField(\"date\",     DateType(),   False), \\\n",
    "    StructField(\"competitor_name\",     StringType(), True), \\\n",
    "    StructField(\"competitor_sku_name\", StringType(), True), \\\n",
    "    StructField(\"competitor_price\",    DoubleType(), True), \\\n",
    "    StructField(\"special_price\",       DoubleType(), True), \\\n",
    "    StructField(\"competitor_url\",      StringType(), True), \\\n",
    "    StructField(\"gift_or_extra_prod\",  StringType(), True)\n",
    "  ])\n",
    "df_scrap = spark.createDataFrame(df_scrap, schema=schema)\n",
    "\n",
    "# TODO: Check how to manage multiple countries\n",
    "# por ahora mantengo sólo lo de MX\n",
    "df_scrap = df_scrap.filter(df_scrap.country == country)\n",
    "\n",
    "# Text cleaning phase\n",
    "df_scrap = df_scrap.withColumn('comp_sku_name_clean', \n",
    "            normalize_text_udf(df_scrap['competitor_sku_name']))\n",
    "            \n",
    "df_scrap.cache()#.groupby('competitor_name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af3e955-9566-45ab-bcbd-3cdf836f8de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Perform the text-cleaning phase of web scraping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aba85a6-a2e6-41c8-9202-fafe2cb02a70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get only the cleaned sku names\n",
    "df_comp_names = df_scrap[['comp_sku_name_clean']].dropDuplicates()\n",
    "df_comp_names.cache()#.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdca76c-b17e-4e6d-ac66-2a9cdbf32751",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SKU matching phase\n",
    "\n",
    "### Number of evaluations to perform\n",
    "Calculate the number of evaluations to perform:\n",
    "\n",
    "$$N_{evals} = m * n$$\n",
    "where $m$= *number of Marketplace skus* and $n$ = *number of scraped products*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cf237d-1383-47bb-8cda-3cb17d6c7e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Total de evaluaciones\n",
    "#_n_rows_mkp = df_mktp_cat.count()\n",
    "#_n_rows_wsp = df_comp_names.count()\n",
    "#_n_evals =_n_rows_mkp * _n_rows_wsp\n",
    "#print(f'[{_n_rows_mkp:,} mkp skus] * [{_n_rows_wsp:,} web scraping skus] ')\n",
    "#print(f'= {_n_evals:,} evaluations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7382ce52-e741-41cb-ae7b-1822be89b69a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Perform the matching phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f5fcb99-4156-42f9-8212-73a55167f609",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Define a function to perform the matching phase and keep the best match between a given pair of competitor and marketplace skus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d933ad-808a-4330-b394-f1e70c2b552c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the UDF for confidence matching \n",
    "@F.udf(DoubleType())\n",
    "def get_confidence_udf(mktp_sku_clean_name, comp_sku_clean_name):\n",
    "    # Replace 'get_confidence'\n",
    "    conf = get_confidence(mktp_sku_clean_name, comp_sku_clean_name)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cf76b8-5221-42ce-b82a-5d6ae9f0f423",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform a cross join to obtain all pair combinations of mkp and comp products\n",
    "# Broadcast the smallest table to enhance performance. \n",
    "# Keep only relevant columns to avoid memory overload\n",
    "cj_df = F.broadcast(df_mktp_cat.select([\"sku\", \"mkp_sku_name_clean\"]))\\\n",
    "        .crossJoin(df_comp_names.select([\"comp_sku_name_clean\"]))\n",
    "\n",
    "# Perform the pair-wise evaluation using the 'get_confidence' function\n",
    "cj_df = cj_df.withColumn(\"confidence\", \n",
    "                 get_confidence_udf(F.col('mkp_sku_name_clean'), \n",
    "                                    F.col('comp_sku_name_clean')))\n",
    "\n",
    "# Now get the best mktp match for each competitor sku\n",
    "# Returns the following structure:\n",
    "# sku|mkp_sku_name_clean|scrap_id|comp_sku_name_clean|confidence|\n",
    "w = Window.partitionBy('comp_sku_name_clean')\n",
    "match_df = cj_df\\\n",
    "        .withColumn('best_conf', F.max('confidence').over(w))\\\n",
    "        .where(F.col('confidence') == F.col('best_conf'))\\\n",
    "        .drop('best_conf')\\\n",
    "        .dropDuplicates(['comp_sku_name_clean']) # Keep only one occurrence \n",
    "# Keep only relevant columns\n",
    "match_df = match_df.select(['comp_sku_name_clean', 'sku', 'confidence'])\n",
    "match_df.cache()#.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8be14c2-419b-4732-b2b6-9bb800c7cf9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Format the output and save to `.parquet`\n",
    "\n",
    "1. Keep only relevant matches\n",
    "3. Keep only relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b973c90b-41d2-4f1b-b5ea-f0492c6edf83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confidence interval\n",
    "conf_thr = 0.4\n",
    "save_mode = \"append\"\n",
    "\n",
    "# Merge the df_scrap with the matching mktp_sku using \n",
    "df_matched = df_scrap.join(match_df, ['comp_sku_name_clean'], \"left\")\\\n",
    "                     .filter(F.col('confidence') >= conf_thr)\\\n",
    "                     .drop('comp_sku_name_clean')\\\n",
    "                     .orderBy(F.col('confidence').desc())\n",
    "\n",
    "# Save to cache\n",
    "df_matched.cache()\n",
    "df_matched.write.format(\"delta\")\\\n",
    "          .mode(save_mode)\\\n",
    "          .saveAsTable(f\"{database}.matched_skus_webscraping\")\n",
    "# option(\"overwriteSchema\", \"true\") -- activate when \"overwrite\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f5769c-ebd7-491b-b36d-0890cce5917d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#print(df_matched.count())\n",
    "#df_matched.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04fb1abf-1e22-42fc-96b2-e3820d2609f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unmount the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aca6413-0401-4379-8119-b2d9a4e0cdfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/webscraping\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sku_matching_pyspark_cluster",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "sku_matching_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
